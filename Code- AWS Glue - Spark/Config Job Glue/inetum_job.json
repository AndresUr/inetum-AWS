{
	"jobConfig": {
		"name": "inetum_job",
		"description": "",
		"role": "arn:aws:iam::248189914606:role/service-role/AWSGlueServiceRole-s3-fullacces-glue-au",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "inetum_job.py",
		"scriptLocation": "s3://aws-glue-assets-248189914606-us-west-2/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--additional-python-modules",
				"value": "spacy==3.8.4",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-02-11T20:48:24.404Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-248189914606-us-west-2/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-248189914606-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"dataLineage": false,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport spacy\nimport spacy.cli\n\n\nfrom pyspark.sql.functions import col, explode, when\nfrom pyspark.sql.functions import col, pandas_udf\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StringType\nimport pandas as pd\nfrom pyspark.sql.functions import year\nfrom pyspark.sql.functions import col, monotonically_increasing_id\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, col\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nimport logging\n\n\n# Configurar logger\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nhandler = logging.StreamHandler(sys.stdout)\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\n## @params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n#\n#args = getResolvedOptions(sys.argv, ['param1', 'param2'])\n#param1_value = args['param1']\n#param2_value = args['param2']\n\nspacy.cli.download(\"en_core_web_sm\")\n\n#Instanciamos la clase Spacy para el proceso NLP\nnlp = spacy.load(\"en_core_web_sm\")\n\n#Cadena de coneccion conector jdbc\njdbc_url = \"jdbc:mysql://database-inetum.ctgecwsy06rn.us-west-2.rds.amazonaws.com:3306/inetum\"\njdbc_properties = {\n    \"user\": \"admin\",\n    \"password\": \"Juannicolas3\",\n    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n}\n\n#Definimos el diccionario de las categorias para clasificar el texto de los articulos\ncategories = {\n    \"science\": [\"NASA\", \"astronomy\", \"telescope\", \"research\", \"space exploration\", \"planets\", \"galaxy\", \"physicists\", \"theory\", \"study\", \"astrophysicist\", \"space laboratory\"],\n    \"missions\": [\"launch\", \"rocket\", \"mission\", \"crew\", \"space station\", \"rovers\", \"moon\", \"mars\", \"jupiter\", \"satellites\", \"orbiter\", \"lunar exploration\", \"space travel\"],\n    \"technology\": [\"technology\", \"innovation\", \"reusable rockets\", \"artificial intelligence\", \"automation\", \"sensors\", \"navigation\", \"space systems\", \"advanced materials\"],\n    \"politics\": [\"government\", \"regulation\", \"space policy\", \"funding\", \"legislation\", \"congress\", \"alliances\", \"international agreements\", \"space competition\", \"space industry\", \"corporations\"],\n    \"commercial\": [\"company\", \"SpaceX\", \"Blue Origin\", \"private\", \"commercial launches\", \"private industry\", \"private rocket\", \"investors\", \"business development\", \"space market\"],\n    \"security\": [\"defense\", \"military satellites\", \"national security\", \"cybersecurity\", \"military technology\", \"space warfare\", \"orbital surveillance\", \"space systems\", \"weapon systems\"],\n    \"astronomy\": [\"star\", \"galaxy\", \"telescope\", \"observation\", \"exoplanets\", \"stellar clusters\", \"black holes\", \"nebula\", \"radio telescope\", \"spectroscopy\", \"search for life\"],\n    \"climate and environment\": [\"climate change\", \"space weather\", \"weather satellites\", \"earth observation\", \"thermal waters\", \"global temperature\", \"solar cycle\", \"space climate\", \"solar energy\"],\n    \"mars exploration\": [\"Mars\", \"rovers\", \"landing\", \"explorer\", \"martian surface\", \"water on Mars\", \"Martian atmosphere\", \"mission to Mars\", \"Curiosity\", \"Perseverance\"],\n    \"astronautics\": [\"astronaut\", \"International Space Station\", \"spacelab\", \"crewed mission\", \"life support system\", \"space module\", \"microgravity experiments\"],\n    \"space future\": [\"colonization\", \"terraforming\", \"space life\", \"lunar station\", \"nuclear rockets\", \"interplanetary exploration\", \"space transportation\", \"advanced propulsion systems\"]\n}\n\n#Lectura de nuestros datos almacenados en el bcuket de S3 a travez del crawler y la BD del datacatalog\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\ndef load_df_from_s3():\n    try:\n        logger.info(\"proceso de lectura de los datos desde el bucket de S3\")\n        dyf = glueContext.create_dynamic_frame.from_catalog(database=\"dbflightnews\", table_name=\"source_data_spacenews\",transformation_ctx=\"dyf\")\n        return dyf\n    except Exception as e:\n        logger.error(f\"Error al leer datos desde el bucket de S3 --- {e}\")\n        raise\n\n# Cargar DataFrame con la opcion de reintentos mediante la implementacion de la clase retry\ntry:\n    dyf = load_df_from_s3()\n    logger.info(\"Datos cargados exitosamente desde el bucket de S3\")\nexcept Exception as e:\n    logger.critical(\"Fallo en la lectura de datos desde el bucket de S3 ---{e}\")\n    sys.exit(1)\n    \n        \n\n#Transofrmamos nuestos datos de un DynamicFrame a un pypark dataframe\ndf = dyf.toDF()\n\n#Realziamos un aplanamiento del campo authors ya que es un array, esto con el fin de exraer el campo autor en una columna individual\ndf = df.withColumn(\"author\", explode(col(\"authors\")))\ndf = df.withColumn(\"author_name\", col(\"author\")[\"name\"])\n\n#Definicion de la funcion para poblar dimensiones y tabla de hechos, Guardado en MySQL\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\ndef save_to_mysql(df, table_name,mode):\n    try:\n        logger.info(f\"Intentando guardar datos en MySQL wn la tabla ----{table_name}------\")\n        df.write \\\n            .format(\"jdbc\") \\\n            .option(\"url\", jdbc_url) \\\n            .option(\"dbtable\", table_name) \\\n            .option(\"user\", jdbc_properties[\"user\"]) \\\n            .option(\"password\", jdbc_properties[\"password\"]) \\\n            .option(\"driver\", jdbc_properties[\"driver\"]) \\\n            .mode(mode) \\\n            .save()\n        logger.info(f\"Datos guardados exitosamente en la tabla {table_name}-------\")\n    except Exception as e:\n        logger.error(f\"Error guardando en MySQL en la tabla ---- {table_name}----{e}\")\n        sys.exit(1)\n\n#Definimos las funciones que van a permitir extraer palabras claves,identidades y la clasificacion de los articulos\n\ndef extract_entities(text, entity_type):\n    if not text:\n        return \"\"\n    doc = nlp(text)\n    return \", \".join(set(ent.text for ent in doc.ents if ent.label_ == entity_type))\n    \ndef extract_keywords(text):\n    if not text:\n        return \"\"\n    doc = nlp(text)\n    return \", \".join(set(token.text for token in doc if token.pos_ in ['NOUN', 'ADJ']))\n\ndef classif_text(text):\n    if not text:\n        return \"\"\n    doc = nlp(text)\n    category_scores = {category: 0 for category in categories}\n    \n    for token in doc:\n        token_text = token.text.strip().lower()\n        for category, keywords in categories.items():\n            for keyword in keywords:\n                if keyword.lower() in token_text:\n                    category_scores[category] += 1\n    category = max(category_scores, key=category_scores.get)\n    \n    return category\n\n@pandas_udf(StringType())\ndef extract_org_udf(texts: pd.Series) -> pd.Series:\n    return texts.apply(lambda x: extract_entities(x, \"ORG\"))\n\n@pandas_udf(StringType())\ndef extract_person_udf(texts: pd.Series) -> pd.Series:\n    return texts.apply(lambda x: extract_entities(x, \"PERSON\"))\n\n@pandas_udf(StringType())\ndef extract_place_udf(texts: pd.Series) -> pd.Series:\n    return texts.apply(lambda x: extract_entities(x, \"LOC\"))\n\n@pandas_udf(StringType())\ndef extract_keywords_udf(texts: pd.Series) -> pd.Series:\n    return texts.apply(lambda x: extract_keywords(x))\n\n@pandas_udf(StringType())\ndef classify(texts: pd.Series) -> pd.Series:\n    return texts.apply(lambda x: classif_text(x))\n    \ndf = df.withColumn(\"entity_company\", extract_org_udf(col(\"summary\")))\ndf = df.withColumn(\"entity_people\", extract_person_udf(col(\"summary\")))\ndf = df.withColumn(\"entity_place\", extract_place_udf(col(\"summary\")))\ndf = df.withColumn(\"key_words\", extract_keywords_udf(col(\"summary\")))\ndf = df.withColumn(\"category\", classify(col(\"summary\")))\n\n#Realizamos el filtrado y limpieza de nuestro dataframe final, seleccionando aquellas columnas con las que vamos a trabajar y son de interes para el desarrollo del proceso\ndf_final = df.select(\"id\",\n                     \"summary\",\n                     \"news_site\",\n                     \"published_at\",\n                     \"updated_at\",\n                     \"title\",\n                     \"type\",\n                     \"author_name\",                     \n                     when(col(\"entity_company\").isNull() | (col(\"entity_company\") == \"\"), \"None\").otherwise(col(\"entity_company\")).alias(\"entity_company\"),                     \n                     when(col(\"entity_people\").isNull() | (col(\"entity_people\") == \"\"), \"None\").otherwise(col(\"entity_people\")).alias(\"entity_people\"),                     \n                     when(col(\"entity_place\").isNull() | (col(\"entity_place\") == \"\"), \"None\").otherwise(col(\"entity_place\")).alias(\"entity_place\"),                     \n                     when(col(\"key_words\").isNull() | (col(\"key_words\") == \"\"), \"None\").otherwise(col(\"key_words\")).alias(\"key_words\"),\n                     \"category\")\n                     \ndf_final.show(2)\nprint(\"--------------Tendencias po teas de tiempo--\"*20)\n\n#Tendencias de Temas por Tiempo\ndf_time = df_final.withColumn(\"published_at\", F.to_timestamp(\"published_at\"))\ndf_tend = df_time.groupBy(F.month(\"published_at\").alias(\"month\"), \"category\").count()\ndf_tend = df_tend.orderBy(\"month\", F.desc(\"count\"))\n\ndf_tend.show(2)\nprint(\"-----------------fuentes as activas-----\")\n#Analisis de fuentes mas activas\ndf_sources = df_final.groupBy(\"news_site\").count()\ndf_sources=df_sources.orderBy(F.desc(\"count\"))\ndf_sources.show(2)\n\n#Particion y almacenamiento de datos historicos\ns3_path = \"s3://source-data-spacenews/historical_data/\"\n# Agregar la columna de año para particionar\ndf_time = df_time.withColumn(\"year\", year(\"published_at\"))\n\n# Reintento para escritura en S3\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\ndef save_bucket_s3(df,s3_path):\n    try:\n        logger.info(f\"Proceso de escritura de datos en el bucket de S3 en la ruta {s3_path} de los datos con particion por año\")\n        # Escribir los datos en formato Parquet en S3 con partición por año\n        df.write.mode(\"overwrite\").partitionBy(\"year\").parquet(s3_path)\n        logger.info(\"Datos escritos exitosamente en el bcuket de  S3.\")\n    except Exception as e:\n        logger.error(f\"Error escribiendo en el bucket deS3: {e}\")\n        raise\n\nsave_bucket_s3(df_time,s3_path)       \n\n#Cacheo de datos\ndf_final.cache()\n\n#poblado de la dimension autor\nprint(\"--------------------------------Dimesion autor\")\ndf_authors = df_final.select(\"author_name\").distinct()\nwindow_spec = Window.orderBy(\"author_name\")\ndf_authors = df_authors.withColumn(\"id_autor\", row_number().over(window_spec))\n# Guardar el DataFrame en MySQL\n#save_to_mysql(df_authors,\"autor\",\"append\")\n\n#Poblado de la dimencion category\nprint(\"--------------------------------Dimesion Category\")\ndf_categories = df_final.select(\"category\").distinct()\nwindow_spec = Window.orderBy(\"category\")\ndf_categories = df_categories.withColumn(\"id_category\", row_number().over(window_spec))\n# Guardar el DataFrame en MySQL\n#save_to_mysql(df_categories,\"category\",\"append\")\n\n\n#Poblado de la dimensionsite\nprint(\"--------------------------------Dimesion Site\")\ndf_site = df_final.select(\"news_site\").distinct()\nwindow_spec = Window.orderBy(\"news_site\")\ndf_site = df_site.withColumn(\"id_site\", row_number().over(window_spec))\n# Guardar el DataFrame en MySQL\n#save_to_mysql(df_site,\"site\",\"append\")\n\n\n#pobado de la tabla de hechos\ndf_final.createOrReplaceTempView(\"df_final\")\ndf_site.createOrReplaceTempView(\"site\")\ndf_categories.createOrReplaceTempView(\"category\")\ndf_authors.createOrReplaceTempView(\"autor\")\n\n#Definicioon del Query\nquery=\"\"\"SELECT \n            a.id_autor,\n            s.id_site,\n            c.id_category,\n            COUNT(df.author_name) AS cantidad_noticias_autor,\n            COUNT(*) OVER(PARTITION BY c.id_category) AS cantidad_noticias_categoria,\n            COUNT(*) OVER(PARTITION BY s.id_site) AS cantidad_noticias_site\n            FROM df_final df\n            JOIN autor a ON (df.author_name = a.author_name)\n            JOIN site s ON (df.news_site = s.news_site)\n            JOIN category c ON (df.category = c.category)\n            GROUP BY a.id_autor, s.id_site, c.id_category\"\"\"\n# Ejecutar la consulta en Spark\ndf_hechos = spark.sql(query)\nprint(\"------------------tabla de hechos\")\n# Guardar el DataFrame en MySQL\ntry:\n    save_to_mysql(df_hechos,\"hechos\",\"append\")\nexcept Exception as e:\n    logger.critical(f\"Fallo al guardar datos en la tabla hechos de MySQ----- {e}\")\n    sys.exit(1)\n\n#Analisis sql\n#Tendencias de temas por mes\nprint(\"-------------------Tendencias por mes\")\nquery=\"\"\"SELECT \n        DATE_TRUNC('month', df.published_at) AS month,\n        c.category,\n        COUNT(*) AS tendencia_count\n        FROM df_final df\n        JOIN category c ON (df.category = c.category)\n        GROUP BY month, c.category\n        ORDER BY month, tendencia_count DESC\"\"\"\n# Ejecutar la consulta en Spark\ndf_tendencias = spark.sql(query)\ndf_tendencias.show()\n\n#Fuentes mas influyentesp\nprint(\"-------------------Fuentes mas influyentes\")\nquery=\"\"\"SELECT \n        s.news_site,\n        COUNT(*) AS count_sources\n        FROM df_final df\n        JOIN site s ON (df.news_site = s.news_site)\n        GROUP BY s.news_site\n        ORDER BY count_sources DESC;\"\"\"\n# Ejecutar la consulta en Spark\ndf_FInfluyentes = spark.sql(query)\ndf_FInfluyentes.show()\n\n\njob.commit()"
}